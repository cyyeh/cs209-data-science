{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Standard Section 9:  Feed Forward Neural Networks\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2019**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Tanner<br/>\n",
    "**Section Leaders**: Marios Mattheakis, Abhimanyu (Abhi) Vasishth, Robbert (Rob) Struyven<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to be familiar with the most basic Artificial Neural Network architecture the  Feed-Forward Neural Network (FFNN). \n",
    "\n",
    "Specifically, we will: \n",
    "  \n",
    "1. Quick review the FFNN *anatomy*.\n",
    "2. Design a simple FFNN from the scratch and fit simple toy-datasets.\n",
    "3. Quantify the prediction (fit) by writing a loss function.\n",
    "4. Write a function for the forward pass through an FFNN with a single hidden layer of arbitrary number of hidden neurons.\n",
    "4. Use TensorFlow and Keras to design the previous architectures. \n",
    "5. Use TensorFlow and Keras to train the network (find the optimal network parameters). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages and check the version of your TensorFlow, it should be the version 2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# from pandas import DataFrame\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTANT: Unless you have the \"TF version 2.0.0\"  try the following\n",
    "```pip install --upgrade pip```\n",
    "\n",
    "```pip install tensorflow==2.0.0 ```\n",
    "\n",
    "**OR**\n",
    "\n",
    "```conda install tensorflow=2.0.0```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## 1. Review of the ANN *anatomy*  \n",
    "\n",
    "#### Input, Hidden, and Output layers\n",
    "\n",
    "The **forward** pass through an FFNN  is  a sequence of linear (affine) and nonlinear (activation) operations. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../fig/forward.jpg\" width=\"70%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Design a Feed Forward neural network\n",
    "\n",
    "\n",
    "Let's create a simple FFNN  with one input,  one linear  neuron as output layer, and one hidden layer of arbitrary number of hidden neurons. Get familiar with the forward propagation.\n",
    "- Define a nonlinear function which will be used for activation. \n",
    "- Create an FFNN with one hidden neuron and get familiar with your activation function.\n",
    "- Load the toyDataSet_1.csv and try to fit. \n",
    "- Quantify the fitting by using a loss function.\n",
    "- Make a general function for the forward pass of an FFNN with  one hidden layer of arbitrary number of hidden neurons. Always keep one input and one output. The output is a linear layer (affine transformation).\n",
    "- Load the toyDataSet_1.csv. Design an FFNN with one hidden neurons and fit.\n",
    "- Load the toyDataSet_2.csv. Design an FFNN with two hidden neurons and fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the activation function\n",
    "Here, we use the *Rectified Linear Unit*  (ReLU) function which is defined as $$g(x)=\\max(0,x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g(z: float) -> float :\n",
    "    return  np.maximum(0, z)\n",
    "\n",
    "# or \n",
    "# g = lambda z: np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a ANN with one hidden neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input vector\n",
    "x_train = np.linspace(-1,1,100)\n",
    "\n",
    "# set the network parameters\n",
    "w1 = 1\n",
    "b1 = 0.0\n",
    "w2  = 1\n",
    "b2  = 0.0 \n",
    "\n",
    "# affine operation\n",
    "l1 = w1*x_train + b1\n",
    "\n",
    "# activation\n",
    "h = g(l1)\n",
    "\n",
    "# output linear layer\n",
    "y_train = w2*h+b2\n",
    "\n",
    "\n",
    "plt.plot(x_train, y_train,'-b' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a few cases to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,8])\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "w1,b1,w2,b2 = 1,0,1,0\n",
    "l1 = w1*x_train + b1\n",
    "y_train = w2*g(l1)+b2\n",
    "plt.plot(x_train,y_train,'b')\n",
    "plt.ylim([-1,1])\n",
    "plt.xlim([-1,1])\n",
    "plt.title('w1, b1, w2, b2 = '+ str(w1) + ', ' + str(b1)+ ', '+ str(w2) + ', ' + str(b2))\n",
    "plt.grid('on')\n",
    "#\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "w1,b1,w2,b2 = 1, 0.5, 1,0\n",
    "l1 = w1*x_train + b1\n",
    "y_train = w2*g(l1)+b2\n",
    "plt.plot(x_train,y_train,'b')\n",
    "plt.ylim([-1,1])\n",
    "plt.xlim([-1,1])\n",
    "plt.title('w1, b1, w2, b2 = '+ str(w1) + ', ' + str(b1)+ ', '+ str(w2) + ', ' + str(b2))\n",
    "plt.grid('on')\n",
    "\n",
    "#\n",
    "plt.subplot(2,2,3)\n",
    "w1,b1,w2,b2 = 1,0.5, 1, -0.5\n",
    "l1 = w1*x_train + b1\n",
    "y_train = w2*g(l1)+b2\n",
    "plt.plot(x_train,y_train,'b')\n",
    "plt.ylim([-1,1])\n",
    "plt.xlim([-1,1])\n",
    "plt.title('w1, b1, w2, b2 = '+ str(w1) + ', ' + str(b1)+ ', '+ str(w2) + ', ' + str(b2))\n",
    "plt.grid('on')\n",
    "\n",
    "#\n",
    "plt.subplot(2,2,4)\n",
    "w1,b1,w2,b2 = 1, 0.5, 2, -.5\n",
    "l1 = w1*x_train + b1\n",
    "y_train = w2*g(l1)+b2\n",
    "plt.plot(x_train,y_train,'b')\n",
    "plt.ylim([-1,1])\n",
    "plt.xlim([-1,1])\n",
    "plt.title('w1, b1, w2, b2 = '+ str(w1) + ', ' + str(b1)+ ', '+ str(w2) + ', ' + str(b2))\n",
    "plt.grid('on')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Fit the data\n",
    "Load the toyDataSet_1.csv from the data directory. Fit the data with the above simple FFNN and plot your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toySet_1 = pd.read_csv('../data/toyDataSet_1.csv')\n",
    "\n",
    "x_train = toySet_1['x'].values.reshape(-1,1)\n",
    "y_train = toySet_1['y'].values.reshape(-1,1)\n",
    "\n",
    "plt.plot(x_train, y_train,'.r',label='data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "# set the network parameters\n",
    "\n",
    "\n",
    "# write the network operations: affine-activation-affine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load 'solutions/sol_1.py'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the prediction and the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train,'or',label='data')\n",
    "plt.plot(x_train, y_model,'-b', label='FFNN' )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Loss function\n",
    "Quantify the quality of the fitting by writing a loss function. Mean Square Error (MSE) is a good choice for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mseLoss(y_data, y_prediction):    \n",
    "    return ((y_data - y_prediction)**2).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Loss = mseLoss(y_train,y_model)\n",
    "print('MSE Loss = ', Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass function\n",
    "Write a function for the forward propagation through an FFNN with  one input, one linear output neuron, and one hidden layers with arbitrary number of neurons. \n",
    "\n",
    "General Scheme:\n",
    "- One input vector: $x$\n",
    "- Affine (linear) transformation with $w_{1},~b_{1}$  are the parameter vectors (or $w_{1i},~b_{1i}$):\n",
    "$$l_1 = \\sum_i^\\text{neurons} w_{1i}x+b_{1i} = w^T_1 x + b_1  = w_1 \\cdot x + b_1 = W_1\\cdot X$$\n",
    "- Activation (nonlinear transformation): $$h = g(l_1)$$\n",
    "- Linear Output layer with a vector for weights $w_o$ and a scalar for the bias $b_o$: $$y = w_o^T h+b_o = w_o \\cdot h + b_o = W_o\\cdot H$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myFFNN(X, W1, Wo ):\n",
    "\n",
    "#     input dimensions  = 1\n",
    "#     output dimensions = 1\n",
    "#     hidden layers = 1\n",
    "#     hidden neurons is determined by the size of W1 or W0\n",
    "#     W1 : parameters of first layer \n",
    "#     Wo : parameters of output layer\n",
    "#     parameters :  weights and biases\n",
    "\n",
    "\n",
    "    # Input Layer: \n",
    "    # add a constant column for the biases to the input vector X\n",
    "    ones = np.ones((len(X),1))\n",
    "    l1 = X\n",
    "    l1 = np.append(l1, ones, axis=1)\n",
    "\n",
    "    # hidden layer: Affine and activation\n",
    "    a1 = np.dot(W1, l1.T)\n",
    "    h1 = g(a1)    \n",
    "    \n",
    "    # Output layer (linear layer) (2 steps)\n",
    "    # (a) Add a const column the h1 for the affine transformation\n",
    "    ones = np.ones((len(X),1))    \n",
    "    \n",
    "    H= np.append(h1.T, ones,axis=1).T\n",
    "    # (b) Affine\n",
    "    a = np.dot(Wo,H)\n",
    "    y_hat = a.T\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the previous parameters in our forward propagation function to fit the toyDataSet_1.csv. Poot the resuts and print the associate loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w11 = 2\n",
    "b11 = 0.0\n",
    "w21  = 1\n",
    "b21  = 0.5\n",
    "\n",
    "# make the parameters matrices\n",
    "# First layer\n",
    "W1 = np.array([[w11,b11]])\n",
    "# Output Layer (only one bias term)\n",
    "Wo = np.array([[w21,b21]])\n",
    "\n",
    "# run the model\n",
    "y_model_1 = myFFNN(x_train, W1, Wo )\n",
    "\n",
    "# plot the prediction and the ground truth\n",
    "plt.plot(x_train, y_train,'or',label='data')\n",
    "plt.plot(x_train, y_model_1,'-b', label='FFNN' )\n",
    "plt.legend()\n",
    "\n",
    "# quantify your prediction\n",
    "Loss_1 = mseLoss(y_train,y_model_1)\n",
    "print('MSE Loss = ', Loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Fit a more complex dataset\n",
    "Load the toyDataSet_2.csv from the data directory. Fit the data with your FFNN function and plot your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toySet_2 = pd.read_csv('../data/toyDataSet_2.csv')\n",
    "\n",
    "x_train2 = toySet_2['x'].values.reshape(-1,1)\n",
    "y_train2 = toySet_2['y'].values.reshape(-1,1)\n",
    "\n",
    "plt.plot(x_train2, y_train2,'.r',label='data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "w11 = \n",
    "b11 = \n",
    "\n",
    "w12 = \n",
    "b12 = \n",
    "\n",
    "w21 = \n",
    "w22 = \n",
    "\n",
    "b2  = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load 'solutions/sol_2.py'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model, plot and quantify the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make the parameters matrices\n",
    "# First Layer\n",
    "W1 = np.array([[w11,b11], [w12,b12]])\n",
    "# Output Layer (only one bias term)\n",
    "Wo = np.array([[w21,w22, b2]])\n",
    "\n",
    "\n",
    "# run the model\n",
    "y_model2 = myFFNN(x_train2, W1, Wo )\n",
    "\n",
    "# plot the prediction and the ground truth\n",
    "plt.plot(x_train2, y_train2,'or',label='data')\n",
    "plt.plot(x_train2, y_model2,'-b', label='FFNN' )\n",
    "plt.legend()\n",
    "\n",
    "# quantify your prediction\n",
    "Loss_2 = mseLoss(y_train2,y_model2)\n",
    "print('MSE Loss = ', Loss_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More complicated function\n",
    "Explore more the functions that this simple network can fit by using more neurons. Essentially explore what function it can generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Two Neurons\n",
    "\n",
    "w11 = -.8\n",
    "b11 = -.1\n",
    "\n",
    "w12 = .4\n",
    "b12 = -.1\n",
    "\n",
    "w21  = 1.3\n",
    "w22  = -.8\n",
    "\n",
    "b2  = 0.5\n",
    "\n",
    "# First Layer\n",
    "W1 = np.array([[w11,b11], [w12,b12]])\n",
    "# Output Layer (only one bias term)\n",
    "Wo = np.array([[w21,w22, b2]])\n",
    "\n",
    "\n",
    "# run the model\n",
    "y_model_p = myFFNN(x_train2, W1, Wo )\n",
    "\n",
    "# plot the prediction and the ground truth\n",
    "plt.plot(x_train2, y_model_p,'b', label='FFNN' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Three Neurons\n",
    "w11 = -.1\n",
    "b11 = .3\n",
    "\n",
    "w12 = .9\n",
    "b12 = -.1\n",
    "\n",
    "w13 = .7\n",
    "b13 = -.2\n",
    "\n",
    "\n",
    "w21  = -1.\n",
    "w22  = -.7\n",
    "w33  = .8\n",
    "\n",
    "b2  = 0.25\n",
    "\n",
    "# First Layer\n",
    "W1 = np.array([[w11,b11], [w12,b12], [w13,b13]])\n",
    "# Output Layer (only one bias term)\n",
    "Wo = np.array([[w21,w22,w33, b2]])\n",
    "\n",
    "\n",
    "# run the model\n",
    "y_model_p = myFFNN(x_train2, W1, Wo )\n",
    "\n",
    "# plot the prediction and the ground truth\n",
    "plt.plot(x_train2, y_model_p,'b', label='FFNN' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random numbers between a,b\n",
    "# (b-a) * np.random.random_sample((4, 4)) + a\n",
    "a = -20\n",
    "b = 20\n",
    "\n",
    "# N neurons\n",
    "N = 50\n",
    "\n",
    "# Create random parameter matrices\n",
    "W1 = (b-a) * np.random.random_sample((N, 2)) + a\n",
    "Wo = (b-a) * np.random.random_sample((1, N+1)) + a\n",
    "\n",
    "# make a bigger interval\n",
    "x_train_p2 = np.linspace(-2,2,1000)\n",
    "x_train_p2= x_train_p2.reshape(-1,1)\n",
    "\n",
    "# run the model\n",
    "y_model_p2 = myFFNN(x_train_p2, W1, Wo )\n",
    "\n",
    "# plot the prediction and the ground truth\n",
    "plt.plot(x_train_p2, y_model_p2,'b', label='FFNN' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------------\n",
    "\n",
    "## 3. Tensor Flow and Keras\n",
    "\n",
    "**Keras, Sequential:** [Source]\n",
    "(https://keras.io/models/sequential/)\n",
    "\n",
    "\n",
    "There are many platforms that create the forward propagation (and not only) through more complex and deep architectures. They also provide the backpropagation function for training a network, that is, to find the optimal parameters for a specific task.\n",
    "\n",
    "Here, we use Tensor Flow (TF) and Keras to employ FFNN for regression tasks. \n",
    "- Use Keras to fit the toyDataSet_2 dataset. Tune the weights manually.\n",
    "- Use  TF to find the optimal parameters for the same dataset. Train the FFNN by using backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read again the toyDataSet_2 and define again the  weights in solution2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toySet_2 = pd.read_csv('../data/toyDataSet_2.csv')\n",
    "\n",
    "x_train2 = toySet_2['x'].values.reshape(-1,1)\n",
    "y_train2 = toySet_2['y'].values.reshape(-1,1)\n",
    "\n",
    "# plt.plot(x_train2, y_train2,'.r',label='data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load 'solutions/sol_2.py'\n",
    "w11 = 1\n",
    "b11 = .25\n",
    "\n",
    "w12 = -1\n",
    "b12 = .25\n",
    "\n",
    "w21  = 1\n",
    "w22  = 1\n",
    "\n",
    "b2  = -0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using TensorFlow and Keras make a FFNN to fit the toyDataSet_2. Use the same architecture and weights as previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential(name='My_two_neurons_model_fixedWeights')\n",
    "\n",
    "# hidden layer with 2 neurons (or nodes)\n",
    "model.add(layers.Dense(2, activation='relu', input_shape=(1,)))\n",
    "\n",
    "# output layer, one neuron \n",
    "model.add(layers.Dense(1,  activation='linear'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get weights \n",
    "weights = model.get_weights()\n",
    "print('Initial values of the parameters')\n",
    "print(weights)\n",
    "\n",
    "# hidden layer\n",
    "weights[0][0]=np.array([ w11, w12]) #weights \n",
    "weights[1]=np.array([b11, b12]) # biases\n",
    "# output layer \n",
    "weights[2]=np.array([[w21],[w22]]) # weights\n",
    "weights[3] = np.array([b2])    # bias\n",
    "\n",
    "model.set_weights(weights)\n",
    "\n",
    "print('\\nAfter setting the parameters')\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize and quantify the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_model_tf1 = model.predict(x_train2)\n",
    "\n",
    "# plot the prediction and the ground truth\n",
    "plt.plot(x_train2, y_train2,'or',label='data')\n",
    "plt.plot(x_train2, y_model_tf1,'-b', label='FFNN' )\n",
    "plt.legend()\n",
    "\n",
    "# quantify your prediction\n",
    "Loss_tf1 = mseLoss(y_train2, y_model_tf1)\n",
    "print('MSE Loss = ', Loss_tf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "#### Let TensorFlow to find the optimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back propagation\n",
    "\n",
    "The **backward** pass is the training. It is based on chain rule and updates the parameters. The optimization is done by minimizing the loss function.\n",
    "\n",
    "<img src=\"../fig/dl.jpg\" width=\"70%\"/>\n",
    "\n",
    "#### Baching, stochastic gradient descent, and epochs\n",
    "Shufle and split the entire dataset in mini-batches help to escape from local minima\n",
    "\n",
    "<img src=\"../fig/batching.jpg\" width=\"70%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_t = models.Sequential(name='My_two_neurons_model_training')\n",
    "\n",
    "# hidden layer with 2 neurons (or nodes)\n",
    "model_t.add(layers.Dense(2, activation='relu', input_shape=(1,)))\n",
    "\n",
    "# output layer, one neuron \n",
    "model_t.add(layers.Dense(1,  activation='linear'))\n",
    "\n",
    "# model_t.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model_t.compile(loss='MSE',optimizer=sgd) \n",
    "history = model_t.fit(x_train2, y_train2, epochs=100, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training & validation loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],'b')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_model_t = model_t.predict(x_train2)\n",
    "\n",
    "# plot the prediction and the ground truth\n",
    "plt.plot(x_train2, y_train2,'.r',label='data')\n",
    "plt.plot(x_train2, y_model_t,'b', label='FFNN' )\n",
    "plt.legend()\n",
    "\n",
    "# quantify your prediction\n",
    "Loss_t = mseLoss(y_train2, y_model_t)\n",
    "print('MSE Loss = ', Loss_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_t = model_t.get_weights()\n",
    "\n",
    "print(\"TF weights:\\n\", weights_t)\n",
    "print()\n",
    "print(\"my weights:\\n\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add more neurons and also take a look during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential(name='My_two_neurons_model_training')\n",
    "# hidden layer with 2 neurons (or nodes)\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(1,)))\n",
    "# output layer, one neuron \n",
    "model.add(layers.Dense(1,  activation='linear'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(loss='MSE',optimizer=sgd) \n",
    "\n",
    "def plotting(model_t, ax, title):\n",
    "    y_model_t = model_t.predict(x_train2)\n",
    "    # quantify your prediction\n",
    "    # plot the prediction and the ground truth\n",
    "    ax.plot(x_train2, y_train2,'.r',label='data')\n",
    "    ax.plot(x_train2, y_model_t,'b', label='FFNN' )\n",
    "    ax.legend()\n",
    "    loss = mseLoss(y_train2, y_model_t)\n",
    "    ax.set_title(title + ' - MSE Loss = ' + str(np.round(loss,5)))\n",
    "\n",
    "f, ax = plt.subplots(2,3, figsize=(16,7.5))\n",
    "ax = ax.ravel()\n",
    "plotting(model, ax[0], 'Epoch 0')\n",
    "for i in range(5):\n",
    "    model.fit(x_train2, y_train2, epochs=100, batch_size=32, verbose=0)\n",
    "    plotting(model, ax[i+1], 'Epoch '+str(100*(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit something very nonlinear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toySet_3 = pd.read_csv('../data/toyDataSet_3.csv')\n",
    "\n",
    "x_train3 = toySet_3['x'].values.reshape(-1,1)\n",
    "y_train3 = toySet_3['y'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train3, y_train3,'or')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desing now a FFNN with two-hidden layers with *tanh* activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential(name='MyNet')\n",
    "\n",
    "# hidden layer with 20 neurons (or nodes)\n",
    "model.add(layers.Dense(20, activation='tanh', input_shape=(1,)))\n",
    "# Add another hidden layer of 20 neurons\n",
    "# hidden layer with 20 neurons (or nodes)\n",
    "model.add(layers.Dense(20, activation='tanh'))\n",
    "# output layer, one neuron \n",
    "model.add(layers.Dense(1,  activation='linear'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.02)\n",
    "model.compile(loss='MSE',optimizer=sgd) \n",
    "history = model.fit(x_train3, y_train3, epochs=1000, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log-scale is helpful since the loss decays fast\n",
    "plt.loglog(history.history['loss'],'b')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_model = model.predict(x_train3)\n",
    "\n",
    "# plot the prediction and the ground truth\n",
    "plt.plot(x_train3, y_train3,'.r',label='data')\n",
    "plt.plot(x_train3, y_model,'b', label='FFNN' )\n",
    "plt.legend()\n",
    "\n",
    "# quantify your prediction\n",
    "Loss_t = mseLoss(y_train3, y_model)\n",
    "print('MSE Loss = ', Loss_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## End of Section\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info for the Tensor Flow 2 and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions for running `tf.keras` with Tensorflow 2.0:  \n",
    "\n",
    "1. Create a `conda` virtual environment by cloning an existing one that you know works\n",
    "```\n",
    "conda create --name myclone --clone myenv\n",
    "```\n",
    "\n",
    "2. Go to [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip]) and follow instructions for your machine.\n",
    "\n",
    "All references to Keras should be written as `tf.keras`.  For example: \n",
    "\n",
    "```\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "tf.keras.models.Sequential\n",
    "tf.keras.layers.Dense, tf.keras.layers.Activation, \n",
    "tf.keras.layers.Dropout, tf.keras.layers.Flatten, tf.keras.layers.Reshape\n",
    "tf.keras.optimizers.SGD\n",
    "tf.keras.preprocessing.image.ImageDataGenerator\n",
    "tf.keras.regularizers\n",
    "tf.keras.datasets.mnist   \n",
    "```\n",
    "\n",
    "You could avoid the long names by using\n",
    "```\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "```\n",
    "These imports do not work on some systems, however, because they pick up previous versions of `keras` and `tensorflow`. That is why I avoid them in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
