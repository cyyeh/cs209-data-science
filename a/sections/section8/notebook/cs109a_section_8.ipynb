{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Standard Section 8: Review Trees and Boosting including Ada Boosting Gradient Boosting and XGBoost.\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2019**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Tanner<br/>\n",
    "**Section Leaders**: Marios Mattheakis, Abhimanyu (Abhi) Vasishth, Robbert (Rob) Struyven<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will work with a spam email dataset again. Our ultimate goal is to be able to build models so that we can predict whether an email is spam or not spam based on word characteristics within each email. We will review Decision Trees, Bagging, and Random Forest methods, and introduce Boosting: Ada Boost and XGBoost.\n",
    "\n",
    "Specifically, we will: \n",
    "  \n",
    "1. *Quick review of last week*  \n",
    "2. Rebuild the Decision Tree model, Bagging model, Random Forest Model just for comparison with Boosting. \n",
    "3. *Theory:* What is Boosting?\n",
    "4. Use the Adaboost on the Spam Dataset.\n",
    "5. *Theory:* What is Gradient Boosting and XGBoost?\n",
    "6. Use XGBoost on the Spam Dataset: Extreme Gradient Boosting\n",
    "\n",
    "Optional: Example to better understand Bias vs Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## 1. *Quick review of last week*  \n",
    "\n",
    "#### The Idea: Decision Trees are just flowcharts and interpretable!\n",
    "\n",
    "It turns out that simple flow charts can be formulated as mathematical models for classification and these models have the properties we desire;\n",
    " - interpretable by humans \n",
    " - have sufficiently complex decision boundaries \n",
    " - the decision boundaries are locally linear, each component of the decision boundary is simple to describe mathematically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "#### How to build Decision Trees (the Learning Algorithm in words): \n",
    "To learn a decision tree model, we take a greedy approach: \n",
    " 1. Start with an empty decision tree (undivided feature space) \n",
    " 2. Choose the ‚Äòoptimal‚Äô predictor on which to split and choose the ‚Äòoptimal‚Äô threshold value for splitting by applying a **splitting criterion (1)**\n",
    " 3. Recurse on on each new node until **stopping condition (2)** is met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we need a (1) splitting criterion and a (2) stopping condition:\n",
    "\n",
    "  #### (1) Splitting criterion \n",
    "\n",
    "<img src=\"data/split2_adj.png\" alt=\"split2\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Stopping condition\n",
    "\n",
    "**Not stopping while building a deeper and deeper tree = 100% training accuracy; Yet we will overfit! \n",
    "\n",
    "To prevent the **overfitting** from happening, we should have stopping condition.\n",
    "\n",
    "-------------\n",
    "\n",
    "#### How do we go from Classification to Regression?\n",
    "\n",
    "- For classification, we return the majority class in the points of each leaf node. \n",
    "- For regression we return the average of the outputs for the points in each leaf node. \n",
    "\n",
    "-------------\n",
    "\n",
    "#### What is bagging?\n",
    "  \n",
    "One way to adjust for the high variance of the output of an experiment is to perform the experiment multiple times and then average the results. \n",
    "\n",
    " 1. **Bootstrap:** we generate multiple samples of training data, via bootstrapping. We train a full decision tree on each sample of data. \n",
    " 2. **AGgregatiING** for a given input, we output the averaged outputs of all the models for that input. \n",
    " \n",
    "This method is called **Bagging: B** ootstrap + **AGG**regat**ING**. \n",
    "\n",
    "-------------\n",
    "\n",
    "#### What is Random Forest? \n",
    "\n",
    "- **Many trees** make a **forest**.\n",
    "- **Many random trees** make a **random forest**.\n",
    "\n",
    "\n",
    "Random Forest is a modified form of bagging that creates ensembles of independent decision trees. \n",
    "To *de-correlate the trees*, we: \n",
    "1. train each tree on a separate bootstrap **random sample** of the full training set (same as in bagging) \n",
    "2. for each tree, at each split, we **randomly select a set of ùêΩ‚Ä≤ predictors from the full set of predictors.** (not done in bagging)\n",
    "3. From amongst the ùêΩ‚Ä≤  predictors, we select the optimal predictor and the optimal corresponding threshold for the split. \n",
    "\n",
    "\n",
    "-------------\n",
    "\n",
    "#### Interesting Piazza post: why randomness in simple decision tree?\n",
    " \n",
    " ```\"Hi there. I notice that there is a parameter called \"random_state\" in decision tree function and I wonder why we need randomness in simple decision tree. If we add randomness in such case, isn't it the same as random forest?\"```\n",
    " \n",
    "  - The problem of learning an optimal decision tree is known to be **NP-complete** under several aspects of optimality and even for simple concepts. \n",
    "  - Consequently, practical decision-tree learning algorithms are based on **heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node**. \n",
    "  - Such algorithms **cannot guarantee to return the globally optimal decision tree**. \n",
    "  - This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement (Bagging).\n",
    "  \n",
    "For example: **What is the defaulth DecisionTreeClassifier behaviour when there are 2 or more best features for a certain split (a tie among \"splitters\")?** (after a deep dive and internet search [link](https://github.com/scikit-learn/scikit-learn/issues/12259 ) ):\n",
    "\n",
    "  - The current default behaviour when splitter=\"best\" is to shuffle the features at each step and take the best feature to split. \n",
    "  - In case there is a tie, we take a random one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------------\n",
    "\n",
    "## 2. Just re-building the tree models of last week\n",
    "\n",
    "### Rebuild the Decision Tree model, Bagging model and Random Forest Model for comparison with Boosting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with a spam email dataset. The dataset has 57 predictors with a response variable called `Spam` that indicates whether an email is spam or not spam. **The goal is to be able to create a classifier or method that acts as a spam filter.**\n",
    "\n",
    "Link to description : https://archive.ics.uci.edu/ml/datasets/spambase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dataframe and Set Column Names\n",
    "spam_df = pd.read_csv('data/spam.csv', header=None)\n",
    "columns = [\"Column_\"+str(i+1) for i in range(spam_df.shape[1]-1)] + ['Spam']\n",
    "spam_df.columns = columns\n",
    "display(spam_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us split the dataset into a 70-30 split by using the following:\n",
    "#Split data into train and test\n",
    "np.random.seed(42)\n",
    "msk = np.random.rand(len(spam_df)) < 0.7\n",
    "data_train = spam_df[msk]\n",
    "data_test = spam_df[~msk]\n",
    "\n",
    "#Split predictor and response columns\n",
    "x_train, y_train = data_train.drop(['Spam'], axis=1), data_train['Spam']\n",
    "x_test , y_test  = data_test.drop(['Spam'] , axis=1), data_test['Spam']\n",
    "\n",
    "print(\"Shape of Training Set :\",data_train.shape)\n",
    "print(\"Shape of Testing Set :\" ,data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Percentage of Spam in Train and Test Set\n",
    "percentage_spam_training = 100*y_train.sum()/len(y_train)\n",
    "percentage_spam_testing  = 100*y_test.sum()/len(y_test)\n",
    "                                                  \n",
    "print(\"Percentage of Spam in Training Set \\t : {:0.2f}%.\".format(percentage_spam_training))\n",
    "print(\"Percentage of Spam in Testing Set \\t : {:0.2f}%.\".format(percentage_spam_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "### Fitting an Optimal Single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Best depth for single decision trees of last week\n",
    "best_depth = 7\n",
    "print(\"The best depth was found to be:\", best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalaute the performance at the best depth\n",
    "model_tree = DecisionTreeClassifier(max_depth=best_depth)\n",
    "model_tree.fit(x_train, y_train)\n",
    "\n",
    "#Check Accuracy of Spam Detection in Train and Test Set\n",
    "acc_trees_training = accuracy_score(y_train, model_tree.predict(x_train))\n",
    "acc_trees_testing  = accuracy_score(y_test,  model_tree.predict(x_test))\n",
    "\n",
    "print(\"Simple Decision Trees: Accuracy, Training Set \\t : {:.2%}\".format(acc_trees_training))\n",
    "print(\"Simple Decision Trees: Accuracy, Testing Set \\t : {:.2%}\".format(acc_trees_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------\n",
    "\n",
    "### Fitting 100 Single Decision Trees while Bagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 100 # we tried a variety of numbers here\n",
    "\n",
    "#Creating model\n",
    "np.random.seed(0)\n",
    "model = DecisionTreeClassifier(max_depth=best_depth+5)\n",
    "\n",
    "#Initializing variables\n",
    "predictions_train = np.zeros((data_train.shape[0], n_trees))\n",
    "predictions_test = np.zeros((data_test.shape[0], n_trees))\n",
    "\n",
    "#Conduct bootstraping iterations\n",
    "for i in range(n_trees):\n",
    "    temp = data_train.sample(frac=1, replace=True)\n",
    "    response_variable = temp['Spam']\n",
    "    temp = temp.drop(['Spam'], axis=1)\n",
    "    \n",
    "    model.fit(temp, response_variable)  \n",
    "    predictions_train[:,i] = model.predict(x_train)   \n",
    "    predictions_test[:,i] = model.predict(x_test)\n",
    "    \n",
    "#Make Predictions Dataframe\n",
    "columns = [\"Bootstrap-Model_\"+str(i+1) for i in range(n_trees)]\n",
    "predictions_train = pd.DataFrame(predictions_train, columns=columns)\n",
    "predictions_test = pd.DataFrame(predictions_test, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ensemble the prediction of each bagged decision tree model\n",
    "def get_prediction(df, count=-1):\n",
    "    count = df.shape[1] if count==-1 else count\n",
    "    temp = df.iloc[:,0:count]\n",
    "    return np.mean(temp, axis=1)>0.5\n",
    "\n",
    "#Check Accuracy of Spam Detection in Train and Test Set\n",
    "acc_bagging_training = 100*accuracy_score(y_train, get_prediction(predictions_train, count=-1))\n",
    "acc_bagging_testing  = 100*accuracy_score(y_test, get_prediction(predictions_test, count=-1))\n",
    "\n",
    "print(\"Bagging: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_bagging_training))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a Random Forest Model\n",
    "#Training\n",
    "model = RandomForestClassifier(n_estimators=n_trees, max_depth=best_depth+5)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "#Performance Evaluation\n",
    "acc_random_forest_training = accuracy_score(y_train, y_pred_train)*100\n",
    "acc_random_forest_testing = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Random Forest: Accuracy, Training Set : {:0.2f}%\".format(acc_random_forest_training))\n",
    "print(\"Random Forest: Accuracy, Testing Set :  {:0.2f}%\".format(acc_random_forest_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's compare the performance of our 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Trees:\\tAccuracy, Training Set \\t: {:.2%}\".format(acc_trees_training))\n",
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "\n",
    "print(\"\\nBagging: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_bagging_training))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "\n",
    "print(\"\\nRandom Forest: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_random_forest_training))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. *Theory:* What is Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bagging and Random Forest:**\n",
    "  - complex and deep trees **overfit**\n",
    "  - thus **let's perform variance reduction on complex trees!**\n",
    "- **Boosting:** \n",
    "  - simple and shallow trees **underfit** \n",
    "  - thus **let's perform bias reduction of simple trees!**\n",
    "  - make the simple trees more expressive!\n",
    "  \n",
    "**Boosting** attempts to improve the predictive flexibility of simple models.\n",
    " - It trains a **large number of ‚Äúweak‚Äù learners in sequence**.\n",
    " - A weak learner is a constrained model (limit the max depth of each decision tree).\n",
    " - Each one in the sequence focuses on **learning from the mistakes** of the one before it.\n",
    " - By more heavily weighting in the mistakes in the next tree, our next tree will learn from the mistakes.\n",
    " - A combining all the weak learners into a single strong learner = **a boosted tree**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/gradient_boosting1.png?\" alt=\"tree_adj\" width=\"70%\"/>\n",
    "\n",
    "----------\n",
    "\n",
    "### Illustrative example (from [source](https://towardsdatascience.com/underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6fe4a8a49dbf))\n",
    "\n",
    "<img src=\"data/boosting.png\" alt=\"tree_adj\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built multiple trees consecutively: Tree 1 -> Tree 2 -> Tree 3 - > ....\n",
    "\n",
    "**The size of the plus or minus singns indicates the weights of a data points for every Tree**. How do we determine these weights?\n",
    "\n",
    "For each consecutive tree and iteration we do the following:\n",
    " - The **wrongly classified data points (\"mistakes\" = red circles)** are identified and **more heavily weighted in the next tree (green arrow)**. \n",
    " - Thus the size of the plus or minus changes in the next tree\n",
    " - This change in weights will influence and change the next simple decision tree\n",
    " - The **correct predictions are** identified and **less heavily weighted in the next tree**.\n",
    "\n",
    "We iterate this process for a certain number of times, stop and construct our final model: \n",
    "- The ensemble (**\"Final: Combination\"**) is a linear combination of the simple trees, and is more expressive!\n",
    "- The ensemble (**\"Final: Combination\"**) has indeed not just one simple decision boundary line, and fits the data better.\n",
    " \n",
    " \n",
    "<img src=\"data/boosting_2.png?\" alt=\"tree_adj\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ada Boost?\n",
    "\n",
    "- Ada Boost = Adaptive Boosting.\n",
    "- AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers\n",
    "\n",
    "<img src=\"data/AdaBoost1.png\" alt=\"tree_adj\" width=\"70%\"/>\n",
    "<img src=\"data/AdaBoost2.png\" alt=\"tree_adj\" width=\"70%\"/>\n",
    "<img src=\"data/AdaBoost3.png\" alt=\"tree_adj\" width=\"70%\"/>\n",
    "\n",
    "**Notice that when $\\hat{y}_n = ùë¶_n$, the weight $w_n$ is small; when $\\hat{y}_n \\neq ùë¶_n$, the weight $w_n$ is larger.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrative Example (from slides)\n",
    "------\n",
    "**Step1. Start with equal distribition initially**\n",
    "<img src=\"data/ADA2.png\" alt=\"tree_adj\" width=\"40%\">\n",
    "\n",
    "------\n",
    "**Step2. Fit a simple classifier**\n",
    "<img src=\"data/ADA3.png\" alt=\"tree_adj\" width=\"40%\"/>\n",
    "\n",
    "------\n",
    "**Step3. Update the weights**\n",
    "<img src=\"data/ADA4.png\" alt=\"tree_adj\" width=\"40%\"/>\n",
    "\n",
    "**Step4. Update the classifier:** First time trivial (we have no model yet.)\n",
    "\n",
    "------\n",
    "**Step2. Fit a simple classifier**\n",
    "<img src=\"data/ADA5.png\" alt=\"tree_adj\" width=\"40%\"/>\n",
    "\n",
    "**Step3. Update the weights:** not shown.\n",
    "\n",
    "------\n",
    "**Step4. Update the classifier:**\n",
    "<img src=\"data/ADA6.png\" alt=\"tree_adj\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use the Adaboost method to visualize Bias-Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try Boosting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit an Adaboost Model\n",
    "\n",
    "#Training\n",
    "model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4), \n",
    "                           n_estimators=200, \n",
    "                           learning_rate=0.05)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "#Performance Evaluation\n",
    "acc_boosting_training = accuracy_score(y_train, y_pred_train)*100\n",
    "acc_boosting_test = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Ada Boost:\\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_boosting_training))\n",
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the test and training accuracy evolve with every iteration (tree)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Iteration based score\n",
    "train_scores = list(model.staged_score(x_train,y_train))\n",
    "test_scores = list(model.staged_score(x_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(train_scores,label='train')\n",
    "plt.plot(test_scores,label='test')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Variation of Accuracy with Iterations - ADA Boost\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))\n",
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost seems to be performing better than Simple Decision Trees and has a similar Test Set Accuracy performance compared to Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random tip:** If a \"for\"-loop takes som time and you want to know the progress while running the loop, use: **tqdm()** ([link](https://github.com/tqdm/tqdm)). No need for 1000's of ```print(i)``` outputs.\n",
    "\n",
    "\n",
    "Usage: ```for i in tqdm( range(start,finish) ):```\n",
    "\n",
    " - tqdm means *\"progress\"* in Arabic (taqadum, ÿ™ŸÇÿØŸëŸÖ) and \n",
    " - tqdm is an abbreviation for *\"I love you so much\"* in Spanish (te quiero demasiado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we change the depth of our AdaBoost trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Timer\n",
    "start = time.time()\n",
    "\n",
    "#Find Optimal Depth of trees for Boosting\n",
    "score_train, score_test, depth_start, depth_end = {}, {}, 2, 30\n",
    "for i in tqdm(range(depth_start, depth_end, 2)):\n",
    "    model = AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=i),\n",
    "        n_estimators=200, learning_rate=0.05)\n",
    "    model.fit(x_train, y_train)\n",
    "    score_train[i] = accuracy_score(y_train, model.predict(x_train))\n",
    "    score_test[i] = accuracy_score(y_test, model.predict(x_test))\n",
    "    \n",
    "# Stop Timer\n",
    "end = time.time()\n",
    "elapsed_adaboost = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "lists1 = sorted(score_train.items())\n",
    "lists2 = sorted(score_test.items())\n",
    "x1, y1 = zip(*lists1) \n",
    "x2, y2 = zip(*lists2) \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.title('Variation of Accuracy with Depth - ADA Boost Classifier')\n",
    "plt.plot(x1, y1, 'b-', label='Train')\n",
    "plt.plot(x2, y2, 'g-', label='Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost complexity depends on both the number of estimators and the base estimator. \n",
    "- In the beginning as our model complexity increases (depth 2-3), we first observe a small increase in accuracy.\n",
    "- But as we go further to the right of the graph (**deeper trees**), our model **will overfit the data.**\n",
    "- **REMINDER and validation: Boosting relies on simple trees!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Food for Thought :**\n",
    "- Are **boosted models independent of one another?** Do they need to wait for the previous model's residuals?\n",
    "- Are **bagging or random forest models independent of each other**, can they be trained in a parallel fashion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. *Theory:* What is Gradient Boosting and XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gradient Boosting?\n",
    "\n",
    "To improve its predictions, **gradient boosting looks at the difference between its current approximation, and the known correct target vector, which is called the residual**.\n",
    "\n",
    "The mathematics:\n",
    "\n",
    "- It may be assumed that there is some imperfect model $F_{m}$ \n",
    "- The gradient boosting algorithm improves on $F_{m}$ constructing a new model that adds an estimator $h$ to provide a better model: \n",
    "$$F_{m+1}(x)=F_{m}(x)+h(x)$$\n",
    "\n",
    "- To find $h$, the gradient boosting solution starts with the observation that a perfect **h** would imply\n",
    "\n",
    "$$F_{m+1}(x)=F_{m}(x)+h(x)=y$$\n",
    "\n",
    "- or, equivalently solving for h,\n",
    "\n",
    "$$h(x)=y-F_{m}(x)$$\n",
    "\n",
    "- Therefore, gradient boosting will fit h to the residual $y-F_{m}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/gradient_boosting2.png\" alt=\"tree_adj\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------\n",
    "\n",
    "### XGBoost: [\"Long May She Reign!\"](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)\n",
    "\n",
    "<img src=\"data/kaggle.png\" alt=\"tree_adj\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------\n",
    "\n",
    "### What is XGBoost and why is it so good!?\n",
    " - Based on Gradient Boosting\n",
    " - XGBoost = **eXtreme Gradient Boosting**; refers to the engineering goal to push the limit of computations resources for boosted tree algorithm\n",
    " \n",
    "**Accuracy:**\n",
    " - XGBoost however uses a **more regularized model formalizaiton to control overfitting** (=better performance) by both L1 and L2 regularization.\n",
    " - Tree Pruning methods: more shallow tree will also prevent overfitting\n",
    " - Improved convergence techniques (like early stopping when no improvement is made for X number of iterations)\n",
    " - Built-in Cross-Validaiton\n",
    " \n",
    "**Computing Speed:**\n",
    " - Special Vector and matrix type data structures for faster results.\n",
    " - Parallelized tree building: using all of your CPU cores during training.\n",
    " - Distributed Computing: for training very large models using a cluster of machines.\n",
    " - Cache Optimization of data structures and algorithm: to make best use of hardware.\n",
    "\n",
    "**XGBoost is building boosted trees in parallel? What? How?**\n",
    "- No: Xgboost doesn't run multiple trees in parallel, you need predictions after each tree to update gradients.\n",
    "- Rather it does the parallelization WITHIN a single tree my using openMP to create branches independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use XGBoost: Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's install XGBoost\n",
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create the training and test data\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "# Parameters\n",
    "param = {\n",
    "    'max_depth': best_depth,  # the maximum depth of each tree\n",
    "    'eta': 0.3,               # the training step for each iteration\n",
    "    'silent': 1,              # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 2}           # the number of classes that exist in this datset\n",
    "\n",
    "# Number of training iterations\n",
    "num_round = 200  \n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Train XGBoost\n",
    "bst = xgb.train(param, \n",
    "                dtrain, \n",
    "                num_round, \n",
    "                evals= [(dtrain, 'train')], \n",
    "                early_stopping_rounds=20, # early stopping\n",
    "                verbose_eval=20)\n",
    "\n",
    "\n",
    "# Make prediction training set\n",
    "preds_train = bst.predict(dtrain)\n",
    "best_preds_train = np.asarray([np.argmax(line) for line in preds_train])\n",
    "\n",
    "# Make prediction test set\n",
    "preds_test = bst.predict(dtest)\n",
    "best_preds_test = np.asarray([np.argmax(line) for line in preds_test])\n",
    "\n",
    "# Performance Evaluation \n",
    "acc_XGBoost_training = accuracy_score(y_train, best_preds_train)*100\n",
    "acc_XGBoost_test = accuracy_score(y_test, best_preds_test)*100\n",
    "\n",
    "# Stop Timer\n",
    "end = time.time()\n",
    "elapsed_xgboost = end - start\n",
    "\n",
    "print(\"XGBoost:\\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_XGBoost_training))\n",
    "print(\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_XGBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the accuracy performance: AdaBoost versus XGBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))\n",
    "print(\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_XGBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the computing performance: AdaBoost versus XGBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AdaBoost elapsed time: \\t{:0.2f}s\".format(elapsed_adaboost))\n",
    "print(\"XGBoost elapsed time: \\t{:0.2f}s\".format(elapsed_xgboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we change the depth of our XGBoost trees and compare to Ada Boost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_xgboost(best_depth):\n",
    "    param = {\n",
    "    'max_depth': best_depth,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 2}  # the number of classes that exist in this datset\n",
    "\n",
    "    # the number of training iterations\n",
    "    num_round = 200  \n",
    "\n",
    "    bst = xgb.train(param, \n",
    "                    dtrain, \n",
    "                    num_round, \n",
    "                    evals= [(dtrain, 'train')], \n",
    "                    early_stopping_rounds=20,\n",
    "                    verbose_eval=False)\n",
    "\n",
    "    preds_train = bst.predict(dtrain)\n",
    "    best_preds_train = np.asarray([np.argmax(line) for line in preds_train])\n",
    "    preds_test = bst.predict(dtest)\n",
    "    best_preds_test = np.asarray([np.argmax(line) for line in preds_test])\n",
    "\n",
    "    #Performance Evaluation\n",
    "    XGBoost_training = accuracy_score(y_train, best_preds_train)\n",
    "    XGBoost_test = accuracy_score(y_test, best_preds_test)\n",
    "    \n",
    "    return XGBoost_training, XGBoost_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Optimal Depth of trees for Boosting\n",
    "score_train_xgb, score_test_xgb = {}, {}\n",
    "depth_start, depth_end = 2, 30\n",
    "for i in tqdm(range(depth_start, depth_end, 2)):\n",
    "    XGBoost_training, XGBoost_test = model_xgboost(i)\n",
    "    score_train_xgb[i] = XGBoost_training\n",
    "    score_test_xgb[i] = XGBoost_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot\n",
    "lists1 = sorted(score_train_xgb.items())\n",
    "lists2 = sorted(score_test_xgb.items())\n",
    "x3, y3 = zip(*lists1) \n",
    "x4, y4 = zip(*lists2) \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.title('Variation of Accuracy with Depth - Adaboost & XGBoost Classifier')\n",
    "plt.plot(x1, y1, label='Train Accuracy Ada Boost')\n",
    "plt.plot(x2, y2, label='Test Accuracy Ada Boost')\n",
    "plt.plot(x3, y3, label='Train Accuracy XGBoost')\n",
    "plt.plot(x4, y4, label='Test Accuracy XGBoost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting**: \n",
    "- No real optimal depth of the simple tree for XGBoost, probably a lot of regularization, pruning, or early stopping when using a deep tree at the start.\n",
    "- XGBoost does not seem to overfit when the depth of the tree increases, as opposed to Ada Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the accuracy performances:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))\n",
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))\n",
    "print(\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_XGBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------\n",
    "\n",
    "**Overview of all the tree algorithms:** [Source](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)\n",
    "\n",
    "<img src=\"data/trees.png\" alt=\"tree_adj\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "\n",
    "## End of Section\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Example to better understand Bias vs Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central notion underlying what we've been learning in lectures and sections so far is the trade-off between overfitting and underfitting. If you remember back to Homework 3, we had a model that seemed to represent our data accurately. However, we saw that as we made it more and more accurate on the training set, it did not generalize well to unobserved data.\n",
    "\n",
    "As a different example, in face recognition algorithms, such as that on the iPhone X, a too-accurate model would be unable to identity someone who styled their hair differently that day. The reason is that our model may learn irrelevant features in the training data. On the contrary, an insufficiently trained model would not generalize well either. For example, it was recently reported that a face mask could sufficiently fool the iPhone X.\n",
    "\n",
    "A widely used solution in statistics to reduce overfitting consists of adding structure to the model, with something like regularization. This method favors simpler models during training.\n",
    "\n",
    "The bias-variance dilemma is closely related. \n",
    "- The **bias** of a model quantifies how precise a model is across training sets. \n",
    "- The **variance** quantifies how sensitive the model is to small changes in the training set. \n",
    "- A **robust** model is not overly sensitive to small changes. \n",
    "- **The dilemma involves minimizing both bias and variance**; we want a precise and robust model. Simpler models tend to be less accurate but more robust. Complex models tend to be more accurate but less robust.\n",
    "\n",
    "**How to reduce bias:**\n",
    " - **Use more complex models, more features, less regularization,** ...\n",
    " - **Boosting:** attempts to improve the predictive flexibility of simple models. Boosting uses simple base models and tries to ‚Äúboost‚Äù their aggregate complexity.\n",
    " \n",
    "**How to reduce variance:**\n",
    " - **Early Stopping:** Its rules provide us with guidance as to how many iterations can be run before the learner begins to over-fit.\n",
    " - **Pruning:** Pruning is extensively used while building related models. It simply removes the nodes which add little predictive power for the problem in hand.\n",
    " - **Regularization:** It introduces a cost term for bringing in more features with the objective function. Hence it tries to push the coefficients for many variables to zero and hence reduce cost term.\n",
    " - **Train with more data:** It won‚Äôt work every time, but training with more data can help algorithms detect the signal better.\n",
    " - **Ensembling:** Ensembles are machine learning methods for combining predictions from multiple separate models. For example:\n",
    "   - **Bagging** attempts to reduce the chance of overfitting complex models: Bagging uses complex base models and tries to ‚Äúsmooth out‚Äù their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
